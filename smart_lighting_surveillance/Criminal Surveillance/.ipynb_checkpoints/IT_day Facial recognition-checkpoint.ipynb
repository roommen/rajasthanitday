{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FIR_ID</th>\n",
       "      <th>NAME</th>\n",
       "      <th>CHARGES</th>\n",
       "      <th>LOCATION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Sreekiran A R</td>\n",
       "      <td>Un ethical Hacker</td>\n",
       "      <td>Bangalore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Vivek Kumar</td>\n",
       "      <td>Blackmail</td>\n",
       "      <td>Jaipur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Arpan Naik</td>\n",
       "      <td>Flight Hijacking</td>\n",
       "      <td>Rajasthan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Runcy</td>\n",
       "      <td>Black Money Dealer</td>\n",
       "      <td>Mars</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   FIR_ID           NAME             CHARGES   LOCATION\n",
       "0       1  Sreekiran A R   Un ethical Hacker  Bangalore\n",
       "1       2    Vivek Kumar           Blackmail     Jaipur\n",
       "2       3     Arpan Naik    Flight Hijacking  Rajasthan\n",
       "3       4          Runcy  Black Money Dealer       Mars"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dlib\n",
    "import scipy.misc\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Get Face Detector from dlib\n",
    "# This allows us to detect faces in images\n",
    "face_detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "# Get Pose Predictor from dlib\n",
    "# This allows us to detect landmark points in faces and understand the pose/angle of the face\n",
    "shape_predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')\n",
    "\n",
    "# Get the face recognition model\n",
    "# This is what gives us the face encodings (numbers that identify the face of a particular person)\n",
    "face_recognition_model = dlib.face_recognition_model_v1('dlib_face_recognition_resnet_model_v1.dat')\n",
    "\n",
    "# This is the tolerance for face comparisons\n",
    "# The lower the number - the stricter the comparison\n",
    "# To avoid false matches, use lower value\n",
    "# To avoid false negatives (i.e. faces of the same person doesn't match), use higher value\n",
    "# 0.5-0.6 works well\n",
    "TOLERANCE = 0.5\n",
    "df = pd.read_csv('record.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_face_encodings(path_to_image):\n",
    "    image = scipy.misc.imread(path_to_image)\n",
    "\n",
    "    # Detect faces using the face detector\n",
    "    detected_faces = face_detector(image, 1)\n",
    "\n",
    "    # Get pose/landmarks of those faces\n",
    "    # Will be used as an input to the function that computes face encodings\n",
    "    # This allows the neural network to be able to produce similar numbers for faces of the same people, regardless of camera angle and/or face positioning in the image\n",
    "    shapes_faces = [shape_predictor(image, face) for face in detected_faces]\n",
    "\n",
    "    # For every face detected, compute the face encodings\n",
    "    return [np.array(face_recognition_model.compute_face_descriptor(image, face_pose, 1)) for face_pose in shapes_faces]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare_face_encodings(known_faces, face):\n",
    "    # Finds the difference between each known face and the given face (that we are comparing)\n",
    "    # Calculate norm for the differences with each known face\n",
    "    # Return an array with True/Face values based on whether or not a known face matched with the given face\n",
    "    # A match occurs when the (norm) difference between a known face and the given face is less than or equal to the TOLERANCE value\n",
    "    return (np.linalg.norm(known_faces - face, axis=1) <= TOLERANCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function returns the name of the person whose image matches with the given face (or 'Not Found')\n",
    "# known_faces is a list of face encodings\n",
    "# names is a list of the names of people (in the same order as the face encodings - to match the name with an encoding)\n",
    "# face is the face we are looking for\n",
    "def find_match(known_faces, names, face):\n",
    "    # Call compare_face_encodings to get a list of True/False values indicating whether or not there's a match\n",
    "    matches = compare_face_encodings(known_faces, face)\n",
    "\n",
    "    # Return the name of the first match\n",
    "    count = 0\n",
    "    for match in matches:\n",
    "        if match:\n",
    "            return names[count]\n",
    "        count += 1\n",
    "\n",
    "    # Return not found if no match found\n",
    "    return 'Not Found'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#getting details if match\n",
    "def details(name):\n",
    "    return df[df['Name'] == name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get path to all the known images\n",
    "# Filtering on .jpg extension - so this will only work with JPEG images ending with .jpg\n",
    "image_filenames = filter(lambda x: x.endswith('.jpg'), os.listdir('images/'))\n",
    "\n",
    "# Sort in alphabetical order\n",
    "image_filenames = sorted(image_filenames)\n",
    "\n",
    "# Get full paths to images\n",
    "paths_to_images = ['images/' + x for x in image_filenames]\n",
    "\n",
    "# List of face encodings we have\n",
    "face_encodings = []\n",
    "\n",
    "# Loop over images to get the encoding one by one\n",
    "for path_to_image in paths_to_images:\n",
    "    # Get face encodings from the image\n",
    "    face_encodings_in_image = get_face_encodings(path_to_image)\n",
    "\n",
    "    # Make sure there's exactly one face in the image\n",
    "    if len(face_encodings_in_image) != 1:\n",
    "        print(\"Please change image: \" + path_to_image + \" - it has \" + str(len(face_encodings_in_image)) + \" faces; it can only have one\")\n",
    "        exit()\n",
    "\n",
    "    # Append the face encoding found in that image to the list of face encodings we have\n",
    "    face_encodings.append(get_face_encodings(path_to_image)[0])\n",
    "    #print face_encodings(path_to_image)[0],len(face_encodings(path_to_image)[0]),type(face_encodings(path_to_image)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.most_common(max(head_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "video_capture.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latest\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-9ac12456bbd9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0mminNeighbors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0mminSize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                 \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCASCADE_SCALE_IMAGE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             )\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import sys\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import Counter\n",
    "faceCascade = cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\")\n",
    "from kafka import KafkaConsumer,KafkaProducer\n",
    "consumer = KafkaConsumer('detection',bootstrap_servers =['172.26.42.131:9092'])\n",
    "#consumer2 = KafkaConsumer('motionsensor',bootstrap_servers = ['Extrack.bridgei2i.in:9092'])\n",
    "try:\n",
    "    consumer.poll()\n",
    "    consumer.seek_to_end()\n",
    "except:\n",
    "    print \"latest\"\n",
    "producer =  KafkaProducer(bootstrap_servers='172.26.42.131:9092')\n",
    "found = []\n",
    "head_count = []\n",
    "p = time.time()\n",
    "\n",
    "while True:\n",
    "    msg= next(consumer)\n",
    "    nparr = np.fromstring(msg.value, np.uint8)\n",
    "    flags = cv2.IMREAD_COLOR\n",
    "    frame = cv2.imdecode(nparr,flags)\n",
    "    #converting to Grayscale\n",
    "    gray = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "    #Detecting the faces\n",
    "    faces = faceCascade.detectMultiScale(\n",
    "        gray,\n",
    "        scaleFactor=1.1,\n",
    "        minNeighbors=5,\n",
    "        minSize=(30, 30),\n",
    "        flags=cv2.CASCADE_SCALE_IMAGE\n",
    "    )\n",
    "\n",
    "    head_count.append(len(faces))\n",
    "    #matching each face with known faces\n",
    "    for (x,y,w,h) in faces:\n",
    "        test = frame[y-int(.21*y):y+h+int(.21*y),x-int(.13*x):x+h+int(.13*x),:]\n",
    "        cv2.imwrite('test.jpg',test)\n",
    "        cv2.imwrite('new.jpg',frame)\n",
    "        names = [x[:-4] for x in image_filenames]\n",
    "\n",
    "        try:    \n",
    "            face_encodings_in_image = get_face_encodings(\"test.jpg\")\n",
    "            print type(face_encodings_image[0]),names,tye(names),type(names[0]),face_encodings_image[0].shape\n",
    "            match = find_match(face_encodings, names, face_encodings_in_image[0])\n",
    "            if match:\n",
    "                found.append(match) \n",
    "                if match != 'Not Found':\n",
    "                    cv2.imwrite('criminals/'+str(match)+'.jpg',test)\n",
    "\n",
    "            else:\n",
    "                print \"Not Found\"\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        q = time.time()\n",
    "        if q-p >1:\n",
    "            out = []\n",
    "            found = filter(lambda x:x !='Not Found',found)\n",
    "            data = Counter(found)\n",
    "            output = map(lambda (x,y):x ,data.most_common(max(head_count)))\n",
    "            #print len(output)\n",
    "            #print output,type(output)\n",
    "            for name in output:\n",
    "\n",
    "                sss = details(name)\n",
    "                l = str([sss[key].values[0] for key in sss.keys()])\n",
    "                out.append(l)\n",
    "#                         out = 'Found = 'details(name))+'Head_count = '+str(max(head_count))\n",
    "            producer.send('faceid',str(out)+' Head_count = '+str(max(head_count)))\n",
    "            #print 'Found = ',out,'Head_count = ',max(head_count)\n",
    "\n",
    "\n",
    "            p = time.time()\n",
    "            head_count = []\n",
    "\n",
    "#print data.most_common(hcount.most_common(1)[0][0]),\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l = []\n",
    "for key in sss.keys():\n",
    "    l.append(sss[key].values[0])\n",
    "str(l)\n",
    "pss = str([sss[key].values[0] for key in sss.keys()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'   Employee Id           Name                    Email Id Department  \\\\\\n0          381  Sreekiran A R  sreekiran.ar@bridgei2i.com        IoT   \\n\\n    Designation   Location  \\n0  Data Analyst  Bangalore  '"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(sss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'          381  Sreekiran A R  sreekiran.ar@bridgei2i.com        IoT     Data Analyst  Bangalore  '"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a.split('\\\\\\n0')\n",
    "c = str(b[1]).split('\\n\\n')\n",
    "d = str(b[1]).split('\\n\\n')[0]\n",
    "e = d+((str(c[1])).split('\\n0')[1])\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = str(details(name))\n",
    "                        b = str(details(name)).split('\\\\\\n')\n",
    "                        c = str(b[1]).split('\\n\\n')\n",
    "                        d = str(b[1]).split('\\n\\n')[0]\n",
    "                        e = d+((str(c[1])).split('\\n')[1])\n",
    "                        out.append(filter(lambda x: '\\n' not in x and len(x)>1 ,e.split('  ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'face_encodings_image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-21897a4b0a11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mface_encodings_in_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_face_encodings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test.jpg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mface_encodings_image\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mface_encodings_image\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'face_encodings_image' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
